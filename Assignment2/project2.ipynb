{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c415a22f",
   "metadata": {},
   "source": [
    "# Assignment 2 Cassandra and Spark\n",
    "## Local database: Cassandra\n",
    "\n",
    "Running the cassandra_db, check if it's running:\n",
    "\n",
    "`docker start cassandra_db`\n",
    "`docker ps` to check if its running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cfd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['0.0.0.0'], port = 9042)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f98ba",
   "metadata": {},
   "source": [
    "## 1) Fetch Elhub PRODUCTION_PER_GROUP_MBA_HOUR for 2021\n",
    "\n",
    "The API returns JSON:API formatted responses. We'll request the `price-areas` entity with dataset `PRODUCTION_PER_GROUP_MBA_HOUR` and extract any `productionPerGroupMbaHour` lists found in the response. The code below is defensive and follows `links.next` for pagination if present.\n",
    "\n",
    "Note about timezones and DST: Elhub timestamps are ISO strings â€” we'll parse them with pandas and convert to `Europe/Oslo` which handles summer/winter transitions. If timestamps are timezone-naive, we'll assume UTC and convert; adjust if your data includes local offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = 'https://api.elhub.no/energy-data/v0/price-areas'\n",
    "DATASET = 'PRODUCTION_PER_GROUP_MBA_HOUR'\n",
    "# full year 2021 (ISO8601). Use UTC times or dataset-local times depending on API behaviour\n",
    "START = '2021-01-01T00:00:00Z'\n",
    "END = '2021-12-31T23:00:00Z'\n",
    "\n",
    "def fetch_production_per_group(start=START, end=END, page_size=1000, sleep_between=0.2):\n",
    "    params = {'dataset': DATASET, 'startTime': start, 'endTime': end, 'page[size]': page_size}\n",
    "    url = BASE\n",
    "    all_items = []\n",
    "    while True:\n",
    "        print('Requesting', url, 'params=', params)\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        # collect any top-level lists named productionPerGroupMbaHour\n",
    "        if isinstance(j, dict):\n",
    "            if 'productionPerGroupMbaHour' in j and isinstance(j['productionPerGroupMbaHour'], list):\n",
    "                all_items.extend(j['productionPerGroupMbaHour'])\n",
    "            # JSON:API style: data[].attributes.productionPerGroupMbaHour\n",
    "            for di in j.get('data', []):\n",
    "                attrs = di.get('attributes', {}) if isinstance(di, dict) else {}\n",
    "                if 'productionPerGroupMbaHour' in attrs and isinstance(attrs['productionPerGroupMbaHour'], list):\n",
    "                    all_items.extend(attrs['productionPerGroupMbaHour'])\n",
    "        # pagination via links.next\n",
    "        next_link = None\n",
    "        if isinstance(j, dict):\n",
    "            links = j.get('links', {})\n",
    "            next_link = links.get('next') if isinstance(links, dict) else None\n",
    "        if next_link:\n",
    "            # follow absolute next link\n",
    "            url = next_link\n",
    "            params = {}\n",
    "            time.sleep(sleep_between)\n",
    "            continue\n",
    "        break\n",
    "    return all_items\n",
    "\n",
    "# Fetch (this may take a while for full year) - consider splitting into smaller ranges if needed\n",
    "items = fetch_production_per_group()\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb8398",
   "metadata": {},
   "source": [
    "## 2) Normalize to DataFrame and parse times\n",
    "\n",
    "We'll convert the list of objects to a pandas DataFrame. Typical object fields include: `priceArea`, `productionGroup`, `startTime`, and `quantityKwh`. We'll parse `startTime` to timezone-aware datetimes in Europe/Oslo. If values are missing or the API uses other key names, adjust the normalization accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fab96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "# Defensive normalization: create a DataFrame from list of dicts\n",
    "df = pd.json_normalize(items)\n",
    "print('Columns found:', df.columns.tolist())\n",
    "# Typical columns - try to standardize\n",
    "possible_cols = {\n",
    "    'priceArea': ['priceArea', 'price_area', 'priceAreaCode'],\n",
    "    'productionGroup': ['productionGroup', 'production_group', 'productionGroupCode'],\n",
    "    'startTime': ['startTime', 'start_time', 'time'],\n",
    "    'quantityKwh': ['quantityKwh', 'quantity_kwh', 'quantity']\n",
    "}\n",
    "\n",
    "def pick_col(df, options):\n",
    "    for o in options:\n",
    "        if o in df.columns:\n",
    "            return o\n",
    "    return None\n",
    "cols = {k: pick_col(df, v) for k,v in possible_cols.items()}\n",
    "print('Mapped columns:', cols)\n",
    "# Keep only available columns\n",
    "df_small = df[[c for c in cols.values() if c is not None]].rename(columns={v:k for k,v in cols.items() if v is not None})\n",
    "# parse startTime\n",
    "if 'startTime' in df_small.columns:\n",
    "    df_small['startTime'] = pd.to_datetime(df_small['startTime'], utc=True, errors='coerce')\n",
    "    # convert to Europe/Oslo\n",
    "    df_small['startTime_local'] = df_small['startTime'].dt.tz_convert('Europe/Oslo')\n",
    "else:\n",
    "    print('startTime column not present; check API response schema')\n",
    "# ensure quantity numeric\n",
    "if 'quantityKwh' in df_small.columns:\n",
    "    df_small['quantityKwh'] = pd.to_numeric(df_small['quantityKwh'], errors='coerce')\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc355f",
   "metadata": {},
   "source": [
    "## 3) Write to Cassandra using Spark\n",
    "\n",
    "We create a Spark session configured with the Spark Cassandra connector jar. The exact connector coordinates depend on your Spark/Scala version. The example below uses a commonly used artifact; update the package string if you use different Spark/Scala versions. We also create the keyspace and table (if not exists) using the cassandra-driver before writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from cassandra.cluster import Cluster\n",
    "import math\n",
    "\n",
    "# Spark + Cassandra connector - update version to match your Spark/Scala setup\n",
    "SPARK_PACKAGES = 'com.datastax.spark:spark-cassandra-connector_2.12:3.1.0'\n",
    "spark = SparkSession.builder \\\n",
    ".appName('ElhubToCassandra') \\\n",
    ".config('spark.master', 'local[*]') \\\n",
    ".config('spark.jars.packages', SPARK_PACKAGES) \\\n",
    ".config('spark.cassandra.connection.host', '127.0.0.1') \\\n",
    ".getOrCreate()\n",
    "\n",
    "print('Spark started:', spark)\n",
    "# Create keyspace and table if not exists (Cassandra must be reachable)\n",
    "KEYSPACE = 'elhubb'\n",
    "TABLE = 'production_per_group'\n",
    "if session is not None:\n",
    "    try:\n",
    "        session.execute(f\"CREATE KEYSPACE IF NOT EXISTS {KEYSPACE} WITH replication = {'{'}'class':'SimpleStrategy','replication_factor':1{'}'}\")\n",
    "        session.execute(f\"CREATE TABLE IF NOT EXISTS {KEYSPACE}.{TABLE} (priceArea text, startTime timestamp, productionGroup text, quantityKwh double, PRIMARY KEY ((priceArea), startTime, productionGroup))\")\n",
    "        print('Keyspace/table ensured')\n",
    "    except Exception as e:\n",
    "        print('Could not create keyspace/table:', e)\n",
    "else:\n",
    "    print('No Cassandra session available; ensure Cassandra is running and reachable on 127.0.0.1:9042')\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame and write to Cassandra\n",
    "# Use df_small from earlier; ensure startTime is a timezone-aware datetime in UTC for Cassandra timestamp column\n",
    "if 'df_small' in globals() and len(df_small):\n",
    "    write_df = df_small.copy()\n",
    "    # Cassandra timestamp type expects naive UTC datetimes or pandas.Timestamp with tzinfo removed but representing UTC; convert accordingly\n",
    "    if 'startTime' in write_df.columns:\n",
    "        write_df['startTime'] = write_df['startTime'].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    # Ensure column types\n",
    "    write_df['quantityKwh'] = pd.to_numeric(write_df['quantityKwh'], errors='coerce')\n",
    "    sdf = spark.createDataFrame(write_df)\n",
    "    sdf.write.format('org.apache.spark.sql.cassandra').options(keyspace=KEYSPACE, table=TABLE).mode('append').save()\n",
    "    print('Wrote', sdf.count(), 'rows to Cassandra')\n",
    "else:\n",
    "    print('No data to write; check earlier steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7d8f2",
   "metadata": {},
   "source": [
    "## 4) Read selected columns from Cassandra using Spark and plot\n",
    "\n",
    "We'll read back the columns `priceArea`, `productionGroup`, `startTime`, and `quantityKwh`. Then we'll create:\n",
    "\n",
    "- A pie chart showing total production for the year 2021 for a chosen price area (each slice = productionGroup).\n",
    "- A line plot for the first month (January 2021) for the chosen price area with separate lines per production group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Cassandra\n",
    "df_cass = spark.read.format('org.apache.spark.sql.cassandra').options(keyspace=KEYSPACE, table=TABLE).load()\n",
    "# select and cache\n",
    "df_selected = df_cass.select('priceArea','productionGroup','startTime','quantityKwh').cache()\n",
    "print('Rows in table:', df_selected.count())\n",
    "df_selected.printSchema()\n",
    "\n",
    "# Convert to pandas for plotting (filter by price area first to limit data transferred)\n",
    "chosen_area = 'NO1'  # change to the price area code you want\n",
    "pdf = df_selected.filter(df_selected.priceArea == chosen_area).toPandas()\n",
    "# ensure startTime is datetime in UTC then convert to Europe/Oslo for plotting\n",
    "pdf['startTime'] = pd.to_datetime(pdf['startTime'], utc=True)\n",
    "pdf['startTime_local'] = pdf['startTime'].dt.tz_convert('Europe/Oslo')\n",
    "\n",
    "# Pie chart: total production by productionGroup for the YEAR\n",
    "year_total = pdf.groupby('productionGroup', as_index=False)['quantityKwh'].sum()\n",
    "import plotly.express as px\n",
    "fig_pie = px.pie(year_total, names='productionGroup', values='quantityKwh', title=f'Total production 2021 - {chosen_area}')\n",
    "fig_pie.show()\n",
    "\n",
    "# Line plot: January 2021 - prepare data\n",
    "jan = pdf[(pdf['startTime_local'] >= '2021-01-01') & (pdf['startTime_local'] < '2021-02-01')].copy()\n",
    "if jan.empty:\n",
    "    print('No January data found for', chosen_area)\n",
    "else:\n",
    "    # Pivot so each productionGroup is a column; index by local time for correct DST handling\n",
    "    pivot = jan.pivot_table(index='startTime_local', columns='productionGroup', values='quantityKwh', aggfunc='sum').fillna(0)\n",
    "    fig_line = px.line(pivot.reset_index(), x='startTime_local', y=pivot.columns, title=f'January 2021 production - {chosen_area}')\n",
    "    fig_line.update_xaxes(rangeslider_visible=True)\n",
    "    fig_line.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be0c78",
   "metadata": {},
   "source": [
    "## Notes and troubleshooting\n",
    "\n",
    "- If the API enforces smaller time ranges per request, split the year into monthly ranges and fetch each separately (call `fetch_production_per_group` per-month).\n",
    "- If timestamps are returned without timezone, verify with a small sample and adjust assumptions (we assumed UTC).\n",
    "- Spark Cassandra connector coordinate (`SPARK_PACKAGES`) must match your Spark/Scala version. If you get `NoClassDefFoundError` for the connector, install the compatible connector jar or use the correct `spark.jars.packages` string.\n",
    "- On Windows, running Spark with the connector may require extra steps or running Spark via WSL or a Linux container. Alternatively, write to Cassandra using the `cassandra-driver` in Python in batches as a fallback.\n",
    "\n",
    "If you'd like, I can split the year into monthly fetches in the notebook (recommended for reliability) and provide a small helper that writes directly to Cassandra using the Python driver if Spark setup causes issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
